"Hepatitis".

col_names = ["class", "age", "sex", "steroid", 
"antivirals", "fatigue", "malaise", 
"anorexia", "liver big", "liver firm", "spleen palpable", "spiders", 
"ascites", "varices", "bilirubin", "alk phosphate", 
"sgot", "albumin", "protime", "histology"]
df_hepa = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
		databases/hepatitis/hepatitis.data', names = col_names)
from sklearn.impute import SimpleImputer
imr = SimpleImputer(missing_values = '?', strategy = 'most_frequent')
imr1 = imr.fit(df_hepa)
df_hepatitis = imr1.transform(df_hepa.values)
X, y = df_hepatitis[:,1:], df_hepatitis[:,0]
A = X[np.where(y == 2)]
B = X[np.where(y == 1)]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 1000)


Tập dữ liệu "BUPA-liver".

col_names = ["A1", "A2", "A3", "A4", 
"A5", "A6", "A7"]
df_bupa = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
		databases/liver-disorders/bupa.data', names = col_names)
X, y = df_bupa.iloc[:,:6].values, df_bupa.iloc[:,6].values
A = X[np.where(y == 2)]
B = X[np.where(y == 1)]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 42)


Tập dữ liệu "Heart-statlog".

df_heart_statlog = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/statlog/heart/heart.dat', header = None)
df1 = df_heart_statlog.iloc[:].values
unwanted = ''
df2 = np.zeros((270,14))
for i in range(df1.shape[0]):
	a = df1[i][0].split(' ')
	while unwanted in a:
		a.remove(unwanted)
	df2[i] = np.array(a)
X = df2[:,:13]
y = df2[:,13]
A = X[np.where(y == 1)]
B = X[np.where(y == 2)]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 0)


Tập dữ liệu "Ionosphere".

import random
random.seed(10000)
df_ionosphere = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/ionosphere/ionosphere.data')
X = df_ionosphere.iloc[:,:34].values
y = df_ionosphere.iloc[:,34].values
A = X[np.where(y == 'b')]
B = X[np.where(y == 'g')]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state =1000)


Tập dữ liệu "Spect".

df_train = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/spect/SPECT.train')
df_test = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/spect/SPECT.test')
X = np.vstack((df_train.iloc[:,1:].values, df_test.iloc[:,1:].values))
y = np.hstack((df_train.iloc[:,0].values, df_test.iloc[:,0].values))
A = X[np.where(y == 1)]
B = X[np.where(y == 0)]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 1000)


Tập dữ liệu "Sonar".

df_sonar = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-
databases/undocumented/connectionist-bench/sonar/sonar.all-data', header = None)
X = df_sonar.iloc[:,:60].values
y = df_sonar.iloc[:,60].values
A = X[np.where(y == 'R')]
B = X[np.where(y == 'M')]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 10000)


Tập dữ liệu "Heart-c".

df3_heart = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/heart-disease/processed.cleveland.data', header = None)
df3_heart.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang',
'oldpeak','slope','ca','thal','predicted'] 
from sklearn.impute import SimpleImputer
imr = SimpleImputer(missing_values = '?', strategy = 'most_frequent')
imr3 = imr.fit(df3_heart)
A3 = imr3.transform(df3_heart.values)
X = A3[:,:13]
y = A3[:,13]
B1 = X[np.where(y == 1)]
B2 = X[np.where(y == 2)]
B3 = X[np.where(y == 3)]
B4 = X[np.where(y == 4)]
B = X[np.where(y == 0)]
A = np.vstack((B1, B2, B3, B4))
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 1000)


Tập dữ liệu "Australian".

df_au = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
		databases/statlog/australian/australian.dat')
df1 = np.array(df_au)
df2 = np.zeros((689,15))
for i in range(df1.shape[0]):
	df2[i] = [float(x) for x in df1[i][0].split(' ')]
X = df2[:,:14]
y = df2[:,14]
A = X[np.where(y == 1)]
B = X[np.where(y == 0)]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 1000)


Tập dữ liệu "CMC".

df_cmc = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/cmc/cmc.data', header = None)
df_cmc.columns = ["Wife's age", "Wife's education", "Husband's education", "Number of children ever born", 
"Wife's religion", "Wife's now working?", "Husband's occupation ", 
"Standard-of-living index ", "Media exposure", "Contraceptive method used"]
X, y = df_cmc.iloc[:,:9].values, df_cmc.iloc[:,9].values
A1 = X[np.where(y == 3)]
A2 = X[np.where(y == 2)]
A3 = X[np.where(y == 1)]
A = A2
B = A1
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 42)


Tập dữ liệu "Credit".

col_names = ["A1", "A2", "A3", "A4", 
"A5", "A6", "A7", 
"A8", "A9", "A10", "A11", "A12", "A13", "A14", "A15", "A16"]
df_cre = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/credit-screening/crx.data', names = col_names)
from sklearn.impute import SimpleImputer
imr = SimpleImputer(missing_values = '?', strategy = 'most_frequent')
imr = imr.fit(df_cre)
imputed_data = imr.transform(df_cre.values)
imputed_data = pd.DataFrame(imputed_data, columns = col_names)
df_credit = pd.concat([pd.get_dummies(imputed_data[["A1", "A4", "A5", "A6", "A7", "A9", "A10", "A12", "A13"]]), pd.DataFrame(imputed_data[["A2", "A3", "A8", "A11", "A14", "A15", "A16"]])], axis = 1)
X0 = df_credit.iloc[:,:46].values
X = np.zeros((690,46))
for i in range(X0.shape[0]):
	X[i] = [float(x) for x in X0[i]]
y = df_credit.iloc[:,46].values
A = X[np.where(y == '-')]
B = X[np.where(y == '+')]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state =10000)


Tập dữ liệu "Diabetes".

import random
random.seed(4000)
df_diabetis = pd.read_csv("E:/DATA/XITRUM/NCS/SVM-and-improvements/diabetes_csv.csv")
X, y = df_diabetis.iloc[:,:8].values, df_diabetis.iloc[:,8]
A1 = X[np.where(y == 'tested_positive')]
B1 = X[np.where(y == 'tested_negative')]
indx = None
A = None
B = None
if A1.shape[0] <= B1.shape[0]: 
	indx = list(range(B1.shape[0]))
	B = B1[random.sample(indx, A1.shape[0])]
	A = A1
else:
	indx = list(range(A1.shape[0]))
	A = A1[random.sample(indx, B1.shape[0])]
	B = B1
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 0)


Tập dữ liệu "Flare-solar".

col_names = ["A1", "A2", "A3", "A4", 
"A5", "A6", "A7", 
"A8", "A9", "A10", "A11", "A12", "A13"]
df_solar = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning
			-databases/solar-flare/flare.data2')
df1 = df_solar.iloc[:].values
df2 = []
for i in range(df1.shape[0]):
	df2.append(df1[i][0].split(' '))
df3 = np.array(df2)
df4 = pd.DataFrame(df3)
df4.columns = col_names
df = pd.concat([pd.get_dummies(df4[['A1','A2', 'A3']]),df4[['A4','A5','A6','A7','A8','A9','A10','A11','A12','A13']]], axis =1)
X0 = df.iloc[:,0:23].values
X = np.zeros((X0.shape))
for i in range(X0.shape[0]):
	X[i] = [float(x) for x in X0[i]]
y0 = df.iloc[:,23:26].values
y1 = np.zeros((y0.shape))
for i in range(y0.shape[0]):
	y1[i] = [float(x) for x in y0[i]]   
y = np.zeros(y1.shape[0])
for i in range(y1.shape[0]):
	for j in range(y1.shape[1]):
		if y1[i,j] != 0:
			y[i] = 1

A = X[np.where(y == 0)]
B = X[np.where(y != 0)]
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state = 1000)



Tập dữ liệu "German".

import random
random.seed(10000)
df_german = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning
			-databases/statlog/german/german.data-numeric')
df1 = df_german.iloc[:].values
unwanted = ''
df2 = np.zeros((999,25))
for i in range(df1.shape[0]):
	a = df1[i][0].split(' ')
	while unwanted in a:
		a.remove(unwanted)
	df2[i] = np.array(a)

X = df2[:,:24]
y = df2[:,24]
A1 = X[np.where(y == 1)]
B1 = X[np.where(y == 2)]
indx = None
A = None
B = None
if A1.shape[0] <= B1.shape[0]: 
	indx = list(range(B1.shape[0]))
	B = B1[random.sample(indx, A1.shape[0])]
	A = A1
else:
	indx = list(range(A1.shape[0]))
	A = A1[random.sample(indx, B1.shape[0])]
	B = B1
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.2, random_state = 42)



Tập dữ liệu "Image".

col_names = ["region-centroid-col", "region-centroid-row", "region-pixel-count", "short-line-density-5", 
"short-line-density-2", "vedge-mean", "vegde-sd", 
"hedge-mean", "hedge-sd", "intensity-mean", "rawred-mean", "rawblue-mean", 
"rawgreen-mean", "exred-mean", "exblue-mean", "exgreen-mean", 
"value-mean", "saturatoin-mean", "hue-mean"]
df_image = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/image/segmentation.test', names = col_names)
df_image1 = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-
	databases/image/segmentation.data', names = col_names)
X = np.vstack((df_image.iloc[3:,1:].values,df_image1.iloc[3:,1:].values)) 
y = np.hstack((df_image.iloc[3:,0].values, df_image1.iloc[3:,0]))
A1 = X[np.where(y == 'BRICKFACE')]
A2 = X[np.where(y == 'CEMENT')]
A3 = X[np.where(y == 'FOLIAGE')]
A4 = X[np.where(y == 'GRASS')]
A1234 = np.vstack((A1,A2,A3,A4))
B1 = X[np.where(y == 'PATH')]
B2 = X[np.where(y == 'SKY')]
B3 = X[np.where(y == 'WINDOW')]
B123 = np.vstack((B1,B2,B3))
A = B3.astype(np.float)
B = A3.astype(np.float)
y_A = np.ones(len(A))
y_B = -np.ones(len(B))
AB = np.vstack((A,B))
y_AB = np.hstack((y_A, y_B))
AB_train, AB_test, y_train, y_test = train_test_split(AB, y_AB, test_size = 0.1, random_state =0)

